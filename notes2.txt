
/dists/(suite or codename, e.g. stable|unstable or wheezy|sid)/
/pool/(main|multiverse|restricted|universe)/(a|b|c|....liba|libb|libc...)/(primary package name)/(all package files, e.g. deb, dsc, tar.gz)

[1]/[2]/[3]

[1] Distribution (either the suite or the code name)
[2] Section, Area, or Components
[3] Architecture

Distribution/Area/Architecture

Distribution = Suite + Codename?


s3 uploader will need to know the hashes of all remote index files to know if they should be updated
if the hash has not changed, don't upload

In the architecture-specific "Relase" files: 
	For "Archive:" stanza, suite names ("stable", "testing", "unstable", …) are used in the Debian archive while codenames ("dapper", "feisty", "gutsy", "hardy", "intrepid", …) are used in the Ubuntu archive

The only "hard" files are Packages and Sources

Load JSON into memory, make updates, reserialize as JSON index, compress

To what extent does s3 enter into the picture? If at all? Do we track which files (indexes) have been
updated and then concurrently push to s3?
when adding a package (http server? or command line?) are we responsible for pushing the files to s3, etc...

aptly does all this?
TODO: test and see if aptly could work in our scenario. If so...use it.
