general workflow:
- parse deb|dsc files and check integrity (add-based operations)
- download Releases file and check download integrity? (or perform a list operation)
- download various Packages/Sources files and check download integrity
- append deb|dsc control information to respective Packages/Sources list
- generate Packages/Sources/Release files
- upload deb|dsc|tar.gz files to pool (overwriting anything already there...) (add-based operations)
- upload Packages/Sources/Release[.gpg] files


actual package name/version/architecture is our unique, primary key: http://www.aptly.info/doc/feature/duplicate/
adding a duplicate package will indicate it was skipped, exit code 0
version should be required for remove/link/unlink. It can be a specific version or "all"
  implicit behavior based upon version is too vague and therefore likely to have significant and undesirable consequences


bundle = a set of packages with the same version, e.g. liveaddress-streetapi
all files that are part of a bundle must all have the same version


// this CLI is needs some attention. It allows to much room for error with s3 paths+dist+category
raptr
   add /path/to/set/of/related/files/all/with/same/version to "s3://bucket-name/dir/dir/dir distribution category"
   remove [package-name] [version] from "s3://bucket-name/dir/dir/dir distribution category"
   link [package-name] [version] from "s3://bucket-name/dir/dir/dir distribution category" to "distribution category"
   unlink [package-name] [version] from "s3://bucket-name/dir/dir/dir distribution category"
   purge "s3://bucket-name/dir/dir/dir" # must scan for all Packages and Sources files along with entire pool directory


deb-s3 (ruby) is a good approach, but doesn't support concurrency, source packages, or the standard pool structure
it also has a ton of runtime dependencies we don't want


The following directory structure makes it easy to find a particular bundle and all files related to it for things
like linking and removing. it's also really easy to discover all files that exist as part of a particular bundle.
Pool directory structure: /pool/category/(first letter of bundle)/(bundle name)/(version)/bundle files here


during remove operations, I like the idea of removing bundles (based upon name) as well as removing specific files
based upon the filename (pkg name+verion+arch). only deb's and dsc's can be removed. orig.tar.gz can't be removed
the orig.tar.gz will be nuked during the next purge/sweep/vacuum operation.


What good is the Releases file in each distribution during normal operations? The only reason we might use it
is to discover what Packages|Sources files exist. Thus far, it doesn't appear to be helping us much.


Priorities:
[Now] DEB/DSC file parsing and integrity checks
[Soon?] Release file parsing (to find all Sources|Packages files)
[Soon] Sources|Packages file parsing (to identify all packages and filenames in a distribution)
[Soon] Compression--just use gzip
[Soon] Various hashes, e.g. MD5, SHA1, SHA256?
[Later] Delete behavior (for package bundles and individual files)
[Later] Sweep/Purge/Clean behavior
   (needs to download the Release file for each distribution and parse it
    so that it knows which pool files are in use and which are not
[Last] Release.gpg generation
[Last] Command line interface
[Future] Link and unlink to/from distribution (for package bundles and individual files)
[Future] DryRun

link/unlink operations are tricky because we're crossing distribution boundaries
we're reading from at least TWO different sets of Release+Sources|Packages files
(even though we're using the same pool) and then we're linking them together
we could perform a LIST command at the root of the repo to get a list of all categories
and then get the Release file and finally the Sources|Packages file for each
we need a higher level of abstraction called Distribution that
understands what/how that can work with a backend to get a Release file
and then gets the various Sources|Releases files and parses and understands them
and we can use those to add/remove packages/files to them
(and perhaps they can verify the integrity of the deb|dsc) files that are added?
(perhaps a Distribution instance can verify the integrity of the added files?)




sample raptr configuration file:
{
	"layouts":{
		"named-layout-1":{
			"distributions":[ "production", "staging", "testing" ],
			"categories":[ "liveaddress", "operations", "accounting", "public" ],
			"architectures":[ "amd64", "i386", "source" ]
		},
    "named-layout-2":{
      "distributions":[ "all" ],
      "categories":[ "main" ],
      "architectures":[ "amd64", "i386" ]
    }
	},
	"s3":{
		"replica-1":{ "region":"us-east-1", "bucket":"replica-1", "layout":"named-layout-1" },
		"replica-2":{ "region":"us-west-1", "bucket":"replica-2", "layout":"named-layout-1" },
		"replica-3":{ "region":"us-west-2", "bucket":"replica-3", "layout":"named-layout-1" }
	},
  "filesystem":{
    "local-01":{ "path":"/local/directory","layout":"named-layout-2" }
  },
  "azure":{}
  "google":{}
}


// config file search paths: current working directory, location of raptr executable?,
// ~/.ratpr, and finally /etc/raptr.conf,

by utilizing the above configuration file, we know exactly how each repository is stuctured
without any guessing or scanning. this means that at startup, we can easily retrieve the exact
manifest files necessary for the action at hand

deb|dsc files with ignored architectures according to the configuration file will be
display an explicit warning during the upload phase:
"[WARNING] Skipping ignored architecture some-file_1.0.7_arm64.deb"
(why not just upload all the deb|dsc files regardless of architecture?)
(at some future day, the repo might add arm64 and we could then link with those packages)

We still have the consideration of whether to upload metadata alongside the uploaded files
If we do upload the metadata, we can then call upload distinct from link, but it requires
us to potentially (but *optionally*) issue two commands and also has the effect of a
non-standard file in the repo that we would need to parse and keep up to date
if additional package files were added to that particular version, we would need to download
it, parse it, and upload it.

BUT this manifest could be a good thing because it does tell me about the files
without me needing to download/parse/interpet any of the Sources or Packages files
which can be especially handy in this scenario:

1. pkg_version already exists and is linked to testing
2. someone else decides to upload pkg_version linked to staging (without looking at testing)

In the above scenario, we've got a problem. If we overwrite pkg_version files, that invalidates
"testing" because the hashes are different. If we don't overwrite, then we can't link to staging
because...the hashes might be different. Short of downloading all Packages and Sources file
for every single upload operation (which might get big), we probably need a manifest file
in each pool directory for a given bundle.

if we decided to use event sourcing, everything would hinge on the events.json file for the repo
(which creates lock contention, but that's exactly what we want). we would probably need
to maintain almost the exact same CLI. but we essentially download and re-upload the
json events every single time...and perform concurrency checks on it. while the
file is compressed, it could get big. but it does tell us everything we need to know
with events, we'd know exactly which distributions, categories, and architectures are supported
within a given repo.

solutions:
1. upload a metadata/manifest file probably in JSON? which includes the requisite metadata
   from the various packages that way we know exactly what packages exist and all
   their relevant information
2. when linking that package to a distribution, we retrieve and parse the manifest
   and use it to build the Sources and Packages files

Partial/failed uploads of DSC|.orig|.debian.tar.gz?

Thought: Once a manifest file exists, all package files@version is complete and cannot be
appended to? That's the easiest answer for now. OR any files full package@version not
in the manifest can be uploaded. This allows us to add additional architectures at
a later date.

1. manifest contains amd64+sources @ 1.0.7 (and all corresponding files are there)
2. raptr wants to upload i386 @ 1.0.7 and i386 doesn't exist, therefore:
   a) upload all i386-related debs
   b) update manifest once debs are uploaded

if manifest fails uploads, the i386 debs are orphans and can be overwritten

How to get this under test?
